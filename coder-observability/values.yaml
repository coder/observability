fullnameOverride: null

global:
  zone: svc

  external_scheme: http
  # The external hostname from which k8s services can be accessed in the form of:
  # <external_scheme>:<svc>.<external_zone>
  # e.g.
  # http://dashboards.coder-observability.svc.cluster.local
  external_zone: svc.cluster.local

  # these settings are global so we can parameterise some values which get rendered by subcharts
  postgres:
    # Credentials suggested in https://coder.com/docs/v2/latest/install/database by default
    hostname: localhost
    port: 5432
    username: coder
    password: secret42
    database: coder
    sslmode: disable
    mount_secret:

    alerts:
      groups:
        Basic:
          enabled: true
          delay: 1m
        Notifications:
          enabled: true
          delay: 15m
          thresholds:
            notify: 0.5
            warning: 0.8
            critical: 0.9
        Connections:
          enabled: true
          delay: 5m
          thresholds:
            notify: 0.5
            warning: 0.8
            critical: 0.9

  coder:
    metrics:
      hostname: localhost
      port: 2112
      scrapeInterval: 15s
      additionalLabels:
        job: coder

collector:
  enabled: true
  fullnameOverride: collector
  agent:
    mode: flow
    configMap:
      name: collector-config
      key: config.river
      create: false
    clustering:
      enabled: false
    extraArgs:
      - --disable-reporting=true
    mounts:
      varlog: true
      dockercontainers: true
  controller:
    type: daemonset
    podAnnotations:
      prometheus.io/scrape: "true"
  crds:
    create: false

  withOTLPReceiver: false
  # configuration blocks
  logging: |-
    logging {
      level  = "debug"
      format = "logfmt"
    }
  discovery: |-
    discovery.kubernetes "pods" {
      role = "pod"
      selectors {
       role  = "pod"
      }
    }
  commonRelabellings: |-
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }
    // coalesce the following labels and pick the first value; we'll use this to define the "job" label
    rule {
      source_labels  = ["__meta_kubernetes_pod_label_app_kubernetes_io_component", "app", "__meta_kubernetes_pod_container_name"]
      separator      = "/"
      target_label   = "__meta_app"
      action         = "replace"
      regex          = "^/*([^/]+?)(?:/.*)?$" // split by the delimiter if it exists, we only want the first one
      replacement    = "${1}"
    }
    rule {
      source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_label_app_kubernetes_io_name", "__meta_app"]
      separator     = "/"
      target_label  = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }
    rule {
      regex   = "__meta_kubernetes_pod_label_(statefulset_kubernetes_io_pod_name|controller_revision_hash)"
      action  = "labeldrop"
    }
    rule {
      regex   = "pod_template_generation"
      action  = "labeldrop"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_phase"]
      regex = "Pending|Succeeded|Failed|Completed"
      action = "drop"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "node"
    }
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)"
      replacement = "__param_$1"
    }
  extraBlocks: ""
    # Examples:
    # loki.source.file "tmpfiles" {
    #   targets    = [
    #     {__path__ = "/tmp/foo.txt", "color" = "pink"},
    #     {__path__ = "/tmp/bar.txt", "color" = "blue"},
    #     {__path__ = "/tmp/baz.txt", "color" = "grey"},
    #   ]
    #   forward_to = [loki.write.loki.receiver]
    # }

dashboards:
  enabled: true
  fullnameOverride: dashboards
  useStatefulSet: true
  replicas: 1
  deploymentStrategy:
    type: Recreate  # avoid MultiAttachError for standard-rwo sc
  service:
    enabled: true
  persistence:
    enabled: true
    size: 10Gi
  testFramework:
    enabled: false
  annotations:
    # TODO: this adds annotations to _all_ resources; can we be more specific?
    prometheus.io/scrape: "true"
  dashboardProviders:
    node_exporter.yaml:
      apiVersion: 1
      providers:
        - name: infra
          orgId: 1
          folder: 'Infrastructure'
          type: file
          disableDeletion: false
          editable: false
          options:
            path: /var/lib/grafana/dashboards/infra
        - name: sidecar
          orgId: 1
          type: file
          disableDeletion: false
          editable: false
          options:
            path: /tmp/dashboards
  dashboards:
    # TODO: import dashboards from coder/coder
    infra:
      node-exporter-full:
        gnetId: 1860
        revision: 36
        datasource: metrics
      postgres-database:
        gnetId: 9628
        revision: 7
        datasource: metrics
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: metrics
          type: prometheus
          url: http://metrics.{{ .Release.Namespace }}.{{ $.Values.global.zone }}
          access: proxy
          isDefault: true
          editable: false
        - name: logs
          type: loki
          url: http://logs-gateway.{{ .Release.Namespace }}.{{ $.Values.global.zone }}
          access: proxy
          isDefault: false
          editable: false
        - name: postgres
          type: postgres
          url: '{{ .Values.global.postgres.hostname }}:{{ .Values.global.postgres.port }}'
          user: '{{ .Values.global.postgres.username }}'
          secureJsonData:
            password: '{{ .Values.global.postgres.password }}'
          jsonData:
            sslmode: '{{ .Values.global.postgres.sslmode }}'
          isDefault: false
          editable: false
  admin:
    existingSecret: grafana-admin
    userKey: username
    passwordKey: password
  grafana.ini:
    analytics:
      reporting_enabled: false
    users:
      allow_sign_up: false
    feature_toggles:
      # migrate Angular panels to React
      # see https://grafana.com/docs/grafana/latest/developers/angular_deprecation/angular-plugins/#automatic-migration-of-plugins
      autoMigrateOldPanels: true
    dashboards:
      # mounted configmap will be synced with sidecar
      default_home_dashboard_path: /tmp/dashboards/status-dashboard.json
  sidecar:
    dashboards:
      provider:
        disableDelete: true
        allowUiUpdates: true
      enabled: true
      labelValue: "1"

metrics:
  enabled: true
  server:
    fullnameOverride: metrics
    podAnnotations:
      prometheus.io/scrape: "true"

    global:
      scrape_interval: 15s
    extraArgs:
      log.level: debug
    replicaCount: 1
    statefulSet:
      enabled: true

    retentionSize: 10GB
    persistentVolume:
      enabled: true
      # Note: allowing +2GB breathing room above storage.tsdb.retention.size
      size: 12Gi
    service:
      type: ClusterIP
    extraFlags:
      - web.enable-lifecycle
      - enable-feature=remote-write-receiver
    extraConfigmapMounts:
      - name: alerts
        mountPath: /etc/config/alerts
        configMap: metrics-alerts
        readonly: true

  serverFiles:
    prometheus.yml:
      # disables scraping of metrics by the Prometheus helm chart since this is managed by the collector
      scrape_configs:
      # use custom rule files to be able to render templates (can't do that in values.yaml, unless that value is evaluated by a tpl call)
      rule_files:
        - /etc/config/alerts/postgres.yaml

  testFramework:
    enabled: false

  # enable metric collection from configmap reloader
  configmapReload:
    prometheus:
      extraArgs:
        log-level: all
        listen-address: 0.0.0.0:9091
        watch-interval: 15s
      containerPort: 9091
      extraConfigmapMounts:
        - name: alerts
          mountPath: /etc/config/alerts
          configMap: metrics-alerts
          readonly: true

  alertmanager:
    fullnameOverride: alerts
    enabled: true
    service:
      port: 80
    podAnnotations:
      prometheus.io/scrape: "true"
  kube-state-metrics:
    fullnameOverride: kube-state-metrics
    enabled: true
    podAnnotations:
      prometheus.io/scrape: "true"
  prometheus-node-exporter:
    fullnameOverride: node-exporter
    enabled: true
    podAnnotations:
      prometheus.io/scrape: "true"

  # Disable push gateway
  prometheus-pushgateway:
    enabled: false
logs:
  enabled: true
  nameOverride: logs
  fullnameOverride: logs

  enterprise:
    enabled: false
    adminApi:
      enabled: false
    useExternalLicense: false

  test:
    canaryServiceAddress: "http://logs-canary:3500/metrics"
    enabled: true

  minio:
    enabled: true
    fullnameOverride: logs-storage
    address: logs-storage.{{ .Release.Namespace }}.{{ .Values.global.zone}}:9000
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/path: "/minio/v2/metrics/cluster"
    podLabels:
      app.kubernetes.io/name: "logs-storage"

  loki:
    auth_enabled: false
    commonConfig:
      path_prefix: /var/loki
      replication_factor: 1
    schemaConfig:
      configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: index_
          period: 24h

    rulerConfig:
      remote_write:
        enabled: true
        clients:
          # "fake" is the default username when auth is disabled (unfortunate, I know)
          fake:
            url: http://metrics.{{ .Release.Namespace }}.{{ .Values.global.zone}}/api/v1/write
            headers:
              Source: Loki
            remote_timeout: 30s
      wal:
        dir: /var/loki-ruler-wal
      alertmanager_url: http://alerts.{{ .Release.Namespace }}.{{ .Values.global.zone}}
      enable_api: true
      ring:
        kvstore:
          store: inmemory
      enable_alertmanager_v2: true
      storage:
        type: local
        local:
          directory: /rules
      rule_path: /rules

  lokiCanary:
    enabled: true
    annotations:
      prometheus.io/scrape: "true"

  chunksCache:
    allocatedMemory: 1024
  resultsCache:
    allocatedMemory: 1024

  # disabled scraping of logs by the Loki helm chart since this is managed by the collector
  monitoring:
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    # creates ConfigMaps of dashboards which are discovered via labels
    dashboards:
      enabled: true

  sidecar:
    rules:
      logLevel: DEBUG
      folder: /rules/fake

  gateway:
    replicas: 1
  write:
    podAnnotations:
      prometheus.io/scrape: "true"
    replicas: 1
    extraArgs:
      - -log.level=debug
  read:
    podAnnotations:
      prometheus.io/scrape: "true"
    replicas: 1
  backend:
    podAnnotations:
      prometheus.io/scrape: "true"
    replicas: 1
    extraVolumes:
      - name: ruler-wal
        emptyDir: { }
    extraVolumeMounts:
      - name: ruler-wal
        mountPath: /var/loki-ruler-wal
    extraArgs:
      - -log.level=debug